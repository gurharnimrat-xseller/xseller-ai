# XSELLER.AI Automation (v2)

This repository scaffolds the daily AI news automation platform described in the latest specification. It includes:

- `pipelines/ai-news-shorts.yml` — high-level pipeline spec that mirrors the automation flow.
- `pipelines/run_ai_news.py` — Python runner that fetches AI news, ranks stories, generates summaries, creates social copy + placeholder images, and updates the dashboard queue.
- `app/streamlit_app.py` — Streamlit dashboard with tabs for AI News Shorts, Text Posts, Hook Lab, KPIs, and Outputs.
- `xseller_ai/` — Python package housing reusable modules (RSS fetch, ranking, summarisation, hook generation, social copy, queue management).
- `data/` — Persistent queue/database placeholders consumed by the dashboard and automation.
- `outputs/` — Destination for rendered videos and social assets (one sub-folder per run date).
- `assets/` — Branding assets and video template placeholders.

## Getting Started

1. Create and populate a Python virtual environment.
2. Install dependencies: `pip install -r requirements.txt`.
3. Configure the environment variables listed in `.env` (API keys, Notion, Repurpose.io, etc.).
4. Run `python pipelines/run_ai_news.py` to execute the automation locally. This will create dated folders under `outputs/` and update `data/ai_shorts_queue.json`.
5. Launch the dashboard with `streamlit run app/streamlit_app.py` to review items generated by the pipeline.
6. Wire `pipelines/ai-news-shorts.yml` to your orchestration layer (Codex pipelines, Airflow, etc.) when you are ready for scheduled execution.

## Directory Layout

```
xseller-ai/
├─ pipelines/
│  ├─ ai-news-shorts.yml
│  └─ run_ai_news.py
├─ app/
│  └─ streamlit_app.py
├─ xseller_ai/
│  ├─ __init__.py
│  ├─ hooks.py
│  ├─ queue.py
│  ├─ ranking.py
│  ├─ rss.py
│  ├─ settings.py
│  ├─ social.py
│  └─ summarizer.py
├─ data/
│  ├─ ai_shorts_queue.json
│  ├─ ai_shorts_db.json
│  └─ hooks_lab.csv
├─ outputs/
│  └─ .gitkeep
├─ assets/
│  ├─ xseller_logo.svg
│  └─ templates/
│     └─ kinetic_ai_short.json
│     └─ kinetic_ai_short.json
└─ README.md

## Running the pipeline programmatically

The runner can be imported and triggered from other orchestrators:

```python
from pipelines.run_ai_news import main

def handler(event=None, context=None):
    main()
```

This allows reuse in serverless tasks, cron jobs, or workflow managers.

Outputs are organised as:

```
outputs/YYYY-MM-DD/
├─ video/
│  └─ manifest.json
└─ social/
   ├─ <story>_x.txt
   ├─ <story>_linkedin.txt
   ├─ …
   └─ <story>_instagram.png
```

`data/ai_shorts_queue.json` is updated/merged on each run to keep the Streamlit dashboard in sync.

## Next Steps

- Wire the Python runner or YAML spec to your production scheduler (Codex pipelines, Airflow, Prefect, etc.).
- Swap the placeholder image generation in `pipelines/run_ai_news.py` with your preferred design tool or image API.
- Connect the dashboard approval actions to persistence/back-end APIs for stateful approvals.
- Integrate Repurpose.io or native social APIs to push approved content live automatically.
```

## Next Steps

- Wire the YAML steps to your actual pipeline runner (e.g. Codex pipelines, Airflow, or Prefect).
- Implement modules referenced in the pipeline (`scraper.fetch_rss`, `transforms.normalize_articles`, etc.).
- Connect the dashboard approval actions to persistence/back-end APIs for stateful approvals.
- Hook into Repurpose.io or platform APIs to push approved content live automatically.
